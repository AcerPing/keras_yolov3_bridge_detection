{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "train_Tiny_Yolo_V3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AcerPing/keras_yolov3_bridge_detection/blob/master/train_Tiny_Yolo_V3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qw5nhJlH3Vjf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "ef1b43b3-0d1d-4c88-eb2e-2146e14c93e6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqyk1Y_6ajAR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "62485585-3ccd-4df7-dba4-46a50c844f2f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSku6gwq2mHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "FOLDER_PATH = '/content/drive/My Drive/keras_yolov3_bridge_detection'\n",
        "sys.path.append(FOLDER_PATH)\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "from keras.layers import Input, Lambda\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "from yolo3.model import preprocess_true_boxes, yolo_body, tiny_yolo_body, yolo_loss\n",
        "from yolo3.utils import get_random_data\n",
        "\n",
        "\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyAHLQiJ2mHW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''loads the classes'''\n",
        "def get_classes(classes_path):\n",
        "    with open(classes_path) as f:\n",
        "        class_names = f.readlines()\n",
        "    class_names = [c.strip() for c in class_names]\n",
        "    return class_names\n",
        "\n",
        "'''loads the anchors from a file'''\n",
        "def get_anchors(anchors_path):\n",
        "    with open(anchors_path) as f:\n",
        "        anchors = f.readline()\n",
        "    anchors = [float(x) for x in anchors.split(',')]\n",
        "    return np.array(anchors).reshape(-1, 2)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzsB6f8m2mHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(input_shape, anchors, num_classes, load_pretrained=False, freeze_body=2, weights_path=''):\n",
        "    '''create the training model'''\n",
        "    K.clear_session() # get a new session\n",
        "    image_input = Input(shape=(None, None, 3))\n",
        "    h, w = input_shape\n",
        "    num_anchors = len(anchors)\n",
        "\n",
        "    y_true = [Input(shape=(h//{0:32, 1:16, 2:8}[l], w//{0:32, 1:16, 2:8}[l], \\\n",
        "        num_anchors//3, num_classes+5)) for l in range(3)]\n",
        "\n",
        "    model_body = yolo_body(image_input, num_anchors//3, num_classes)\n",
        "    print(f'Create YOLOv3 model with {num_anchors} anchors and {num_classes} classes.')\n",
        "\n",
        "    if load_pretrained:\n",
        "        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
        "        print('Load weights {}.'.format(weights_path))\n",
        "        if freeze_body in [1, 2]:\n",
        "            # Freeze darknet53 body or freeze all but 3 output layers.\n",
        "            num = (185, len(model_body.layers)-3)[freeze_body-1]\n",
        "            for i in range(num): model_body.layers[i].trainable = False\n",
        "            print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
        "\n",
        "    model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
        "        arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.5})(\n",
        "        [*model_body.output, *y_true])\n",
        "    model = Model([model_body.input, *y_true], model_loss)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Liw9QRBHTs6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_tiny_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2,\n",
        "            weights_path='/content/drive/My Drive/keras_yolov3_bridge_detection/model_data/yolov3-tiny.weights'):\n",
        "    '''create the training model, for Tiny YOLOv3'''\n",
        "    K.clear_session() # get a new session\n",
        "    image_input = Input(shape=(None, None, 3))\n",
        "    h, w = input_shape\n",
        "    num_anchors = len(anchors)\n",
        "\n",
        "    y_true = [Input(shape=(h//{0:32, 1:16}[l], w//{0:32, 1:16}[l], \\\n",
        "        num_anchors//2, num_classes+5)) for l in range(2)]\n",
        "\n",
        "    model_body = tiny_yolo_body(image_input, num_anchors//2, num_classes)\n",
        "    print('Create Tiny YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes))\n",
        "\n",
        "    if load_pretrained:\n",
        "        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
        "        print('Load weights {}.'.format(weights_path))\n",
        "        if freeze_body in [1, 2]:\n",
        "            # Freeze the darknet body or freeze all but 2 output layers.\n",
        "            num = (20, len(model_body.layers)-2)[freeze_body-1]\n",
        "            for i in range(num): model_body.layers[i].trainable = False\n",
        "            print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
        "\n",
        "    model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
        "        arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.7})(\n",
        "        [*model_body.output, *y_true])\n",
        "    model = Model([model_body.input, *y_true], model_loss)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3aVW9eG2mHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''data generator for fit_generator'''\n",
        "def data_generator(folder_path, annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
        "    n = len(annotation_lines)\n",
        "    i = 0\n",
        "    while True:\n",
        "        image_data = []\n",
        "        box_data = []\n",
        "        for b in range(batch_size):\n",
        "            if i==0:\n",
        "                np.random.shuffle(annotation_lines)\n",
        "            image, box = get_random_data(folder_path, annotation_lines[i], input_shape, random=True)\n",
        "            image_data.append(image)\n",
        "            box_data.append(box)\n",
        "            i = (i+1) % n\n",
        "        image_data = np.array(image_data)\n",
        "        box_data = np.array(box_data)\n",
        "        y_true = preprocess_true_boxes(box_data, input_shape, anchors, num_classes)\n",
        "        yield [image_data, *y_true], np.zeros(batch_size)\n",
        "\n",
        "def data_generator_wrapper(folder_path, annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
        "    n = len(annotation_lines)\n",
        "    if n==0 or batch_size<=0: return None\n",
        "    return data_generator(folder_path, annotation_lines, batch_size, input_shape, anchors, num_classes)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIlKwFkK2mHb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "27b81ebb-55ab-46af-b3de-0d00f9a5ebd3"
      },
      "source": [
        "annotation_path = os.path.join(FOLDER_PATH, 'dataset/train_txt/anno.txt')\n",
        "classes_path = os.path.join(FOLDER_PATH, 'model_data/aoi_classes.txt')\n",
        "anchors_path = os.path.join(FOLDER_PATH, 'model_data/tiny_yolo_anchors.txt')\n",
        "class_names = get_classes(classes_path)\n",
        "num_classes = len(class_names)\n",
        "anchors = get_anchors(anchors_path)\n",
        "\n",
        "input_shape = (256,256) # multiple of 32, hw\n",
        "\n",
        "\n",
        "model = create_tiny_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2, weights_path='/content/drive/My Drive/keras_yolov3_bridge_detection/Yolo_V3_Weight_H5_with_Negtive/ep005-loss14.792-val_loss15.959.h5') # make sure you know what you freeze\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)\n",
        "\n",
        "checkpoint = ModelCheckpoint('/content/drive/My Drive/keras_yolov3_bridge_detection/Yolo_V3_Weight_H5_with_Negtive/ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',\n",
        "    monitor='val_loss', save_weights_only=True, save_best_only=True, period=5,  metrics=[\"accuracy\"])\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=2)\n",
        "\n",
        "\n",
        "\n",
        "val_split = 0.1\n",
        "with open(annotation_path) as f:\n",
        "    lines = f.readlines()\n",
        "np.random.seed(5566)\n",
        "np.random.shuffle(lines)\n",
        "np.random.seed(None)\n",
        "num_val = int(len(lines)*val_split)\n",
        "num_train = len(lines) - num_val\n",
        "\n",
        "# Train with frozen layers first, to get a stable loss.\n",
        "# Adjust num epochs to your dataset. This step is enough to obtain a not bad model.\n",
        "if True:\n",
        "    model.compile(optimizer=Adam(lr=1e-3), loss={\n",
        "        # use custom yolo_loss Lambda layer.\n",
        "        'yolo_loss': lambda y_true, y_pred: y_pred})\n",
        "\n",
        "    batch_size = 32\n",
        "    print(f'Train on {num_train} samples, val on {num_val} samples, with batch size {batch_size}.')\n",
        "    model.fit_generator(data_generator_wrapper(FOLDER_PATH, lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
        "            steps_per_epoch=max(1, num_train//batch_size),\n",
        "            validation_data=data_generator_wrapper(FOLDER_PATH, lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
        "            validation_steps=max(1, num_val//batch_size),\n",
        "            epochs=50,\n",
        "            verbose=1,\n",
        "            initial_epoch=0,\n",
        "            callbacks=[checkpoint])\n",
        "    model.save_weights('trained_weights_stage_1.h5')\n",
        "\n",
        "    # model.predict(data_generator_wrapper(FOLDER_PATH, lines[:num_train], batch_size, input_shape, anchors, num_classes),)\n",
        "\n",
        "# Unfreeze and continue training, to fine-tune.\n",
        "# Train longer if the result is not good.\n",
        "if True:\n",
        "    for i in range(len(model.layers)):\n",
        "        model.layers[i].trainable = True\n",
        "    model.compile(optimizer=Adam(lr=1e-4), loss={'yolo_loss': lambda y_true, y_pred: y_pred}) # recompile to apply the change\n",
        "    print('Unfreeze all of the layers.')\n",
        "\n",
        "    batch_size = 16 # note that more GPU memory is required after unfreezing the body\n",
        "    print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
        "    model.fit_generator(data_generator_wrapper(FOLDER_PATH, lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
        "        steps_per_epoch=max(1, num_train//batch_size),\n",
        "        validation_data=data_generator_wrapper(FOLDER_PATH, lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
        "        validation_steps=max(1, num_val//batch_size),\n",
        "        epochs=100,\n",
        "        initial_epoch=50,\n",
        "        callbacks=[checkpoint, reduce_lr, early_stopping])\n",
        "    model.save_weights('trained_weights_final.h5')\n",
        "\n",
        "    \n",
        "\n",
        "# Further training if needed.\n",
        "if True:\n",
        "    for i in range(len(model.layers)):\n",
        "        model.layers[i].trainable = True\n",
        "    model.compile(optimizer=Adam(lr=1e-4), loss={'yolo_loss': lambda y_true, y_pred: y_pred}) # recompile to apply the change\n",
        "    print('Unfreeze all of the layers.')\n",
        "\n",
        "    batch_size = 8 # note that more GPU memory is required after unfreezing the body\n",
        "    print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
        "    model.fit_generator(data_generator_wrapper(FOLDER_PATH, lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
        "        steps_per_epoch=max(1, num_train//batch_size),\n",
        "        validation_data=data_generator_wrapper(FOLDER_PATH, lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
        "        validation_steps=max(1, num_val//batch_size),\n",
        "        epochs=100,\n",
        "        initial_epoch=100,\n",
        "        callbacks=[checkpoint, reduce_lr, early_stopping])\n",
        "    model.save_weights('trained_weights_final.h5')\n",
        "\n",
        "\n",
        "# Further training if needed.\n",
        "if True:\n",
        "    for i in range(len(model.layers)):\n",
        "        model.layers[i].trainable = True\n",
        "    model.compile(optimizer=Adam(lr=1e-4), loss={'yolo_loss': lambda y_true, y_pred: y_pred}) # recompile to apply the change\n",
        "    print('Unfreeze all of the layers.')\n",
        "\n",
        "    batch_size = 4 # note that more GPU memory is required after unfreezing the body\n",
        "    print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
        "    model.fit_generator(data_generator_wrapper(FOLDER_PATH, lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
        "        steps_per_epoch=max(1, num_train//batch_size),\n",
        "        validation_data=data_generator_wrapper(FOLDER_PATH, lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
        "        validation_steps=max(1, num_val//batch_size),\n",
        "        epochs=100,\n",
        "        initial_epoch=150,\n",
        "        callbacks=[checkpoint, reduce_lr, early_stopping])\n",
        "    model.save_weights('trained_weights_final.h5')\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create Tiny YOLOv3 model with 6 anchors and 1 classes.\n",
            "Load weights /content/drive/My Drive/keras_yolov3_bridge_detection/Yolo_V3_Weight_H5_with_Negtive/ep005-loss14.792-val_loss15.959.h5.\n",
            "Freeze the first 42 layers of total 44 layers.\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Train on 6300 samples, val on 700 samples, with batch size 32.\n",
            "Epoch 1/50\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f76d76d6378> and will run it as-is.\n",
            "Cause: could not parse the source code:\n",
            "\n",
            "        'yolo_loss': lambda y_true, y_pred: y_pred})\n",
            "\n",
            "This error may be avoided by creating the lambda in a standalone statement.\n",
            "\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f76d76d6378> and will run it as-is.\n",
            "Cause: could not parse the source code:\n",
            "\n",
            "        'yolo_loss': lambda y_true, y_pred: y_pred})\n",
            "\n",
            "This error may be avoided by creating the lambda in a standalone statement.\n",
            "\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "196/196 [==============================] - 199s 1s/step - loss: 14.5148 - val_loss: 15.1393\n",
            "Epoch 2/50\n",
            "196/196 [==============================] - 199s 1s/step - loss: 14.2270 - val_loss: 14.4418\n",
            "Epoch 3/50\n",
            "196/196 [==============================] - 200s 1s/step - loss: 14.0623 - val_loss: 14.9688\n",
            "Epoch 4/50\n",
            "196/196 [==============================] - 200s 1s/step - loss: 13.9925 - val_loss: 14.5779\n",
            "Epoch 5/50\n",
            "196/196 [==============================] - 199s 1s/step - loss: 14.0121 - val_loss: 14.4793\n",
            "Epoch 6/50\n",
            "196/196 [==============================] - 202s 1s/step - loss: 13.9947 - val_loss: 14.4465\n",
            "Epoch 7/50\n",
            "196/196 [==============================] - 201s 1s/step - loss: 14.0301 - val_loss: 14.8485\n",
            "Epoch 8/50\n",
            "196/196 [==============================] - 199s 1s/step - loss: 13.8798 - val_loss: 14.8268\n",
            "Epoch 9/50\n",
            "196/196 [==============================] - 201s 1s/step - loss: 13.9861 - val_loss: 14.4387\n",
            "Epoch 10/50\n",
            "196/196 [==============================] - 199s 1s/step - loss: 13.8920 - val_loss: 14.6545\n",
            "Epoch 11/50\n",
            "196/196 [==============================] - 199s 1s/step - loss: 13.8705 - val_loss: 14.6924\n",
            "Epoch 12/50\n",
            "196/196 [==============================] - 199s 1s/step - loss: 13.9815 - val_loss: 14.5493\n",
            "Epoch 13/50\n",
            "196/196 [==============================] - 198s 1s/step - loss: 13.8747 - val_loss: 14.5092\n",
            "Epoch 14/50\n",
            "196/196 [==============================] - 198s 1s/step - loss: 14.0320 - val_loss: 14.4490\n",
            "Epoch 15/50\n",
            "196/196 [==============================] - 199s 1s/step - loss: 13.9229 - val_loss: 14.6546\n",
            "Epoch 16/50\n",
            "196/196 [==============================] - 199s 1s/step - loss: 13.8499 - val_loss: 14.6597\n",
            "Epoch 17/50\n",
            "196/196 [==============================] - 199s 1s/step - loss: 13.8639 - val_loss: 14.2659\n",
            "Epoch 18/50\n",
            "196/196 [==============================] - 198s 1s/step - loss: 13.8693 - val_loss: 14.4731\n",
            "Epoch 19/50\n",
            "196/196 [==============================] - 198s 1s/step - loss: 13.8732 - val_loss: 14.2109\n",
            "Epoch 20/50\n",
            "196/196 [==============================] - 197s 1s/step - loss: 13.8391 - val_loss: 14.7078\n",
            "Epoch 21/50\n",
            "196/196 [==============================] - 198s 1s/step - loss: 13.8526 - val_loss: 14.6332\n",
            "Epoch 22/50\n",
            "196/196 [==============================] - 198s 1s/step - loss: 13.8561 - val_loss: 14.5454\n",
            "Epoch 23/50\n",
            "196/196 [==============================] - 200s 1s/step - loss: 14.1266 - val_loss: 14.6341\n",
            "Epoch 24/50\n",
            "196/196 [==============================] - 203s 1s/step - loss: 13.8783 - val_loss: 14.6987\n",
            "Epoch 25/50\n",
            "196/196 [==============================] - 200s 1s/step - loss: 13.8240 - val_loss: 14.6101\n",
            "Epoch 26/50\n",
            "196/196 [==============================] - 199s 1s/step - loss: 14.0019 - val_loss: 13.9023\n",
            "Epoch 27/50\n",
            "196/196 [==============================] - 200s 1s/step - loss: 13.7405 - val_loss: 14.4652\n",
            "Epoch 28/50\n",
            "196/196 [==============================] - 199s 1s/step - loss: 13.7771 - val_loss: 14.3617\n",
            "Epoch 29/50\n",
            "196/196 [==============================] - 198s 1s/step - loss: 13.7863 - val_loss: 14.4321\n",
            "Epoch 30/50\n",
            "196/196 [==============================] - 199s 1s/step - loss: 13.8024 - val_loss: 14.6913\n",
            "Epoch 31/50\n",
            "196/196 [==============================] - 202s 1s/step - loss: 13.7767 - val_loss: 14.7032\n",
            "Epoch 32/50\n",
            "196/196 [==============================] - 202s 1s/step - loss: 13.8974 - val_loss: 14.6777\n",
            "Epoch 33/50\n",
            "196/196 [==============================] - 204s 1s/step - loss: 13.7854 - val_loss: 14.5664\n",
            "Epoch 34/50\n",
            "196/196 [==============================] - 206s 1s/step - loss: 13.9266 - val_loss: 14.2275\n",
            "Epoch 35/50\n",
            "196/196 [==============================] - 205s 1s/step - loss: 13.8106 - val_loss: 14.9570\n",
            "Epoch 36/50\n",
            "196/196 [==============================] - 204s 1s/step - loss: 13.8318 - val_loss: 14.2754\n",
            "Epoch 37/50\n",
            "196/196 [==============================] - 202s 1s/step - loss: 13.9267 - val_loss: 15.0280\n",
            "Epoch 38/50\n",
            "196/196 [==============================] - 204s 1s/step - loss: 13.8001 - val_loss: 14.4550\n",
            "Epoch 39/50\n",
            "196/196 [==============================] - 204s 1s/step - loss: 13.7418 - val_loss: 14.7286\n",
            "Epoch 40/50\n",
            "196/196 [==============================] - 205s 1s/step - loss: 13.8826 - val_loss: 14.6029\n",
            "Epoch 41/50\n",
            "196/196 [==============================] - 205s 1s/step - loss: 13.9196 - val_loss: 13.9165\n",
            "Epoch 42/50\n",
            "196/196 [==============================] - 205s 1s/step - loss: 13.8202 - val_loss: 14.5249\n",
            "Epoch 43/50\n",
            "196/196 [==============================] - 205s 1s/step - loss: 13.8591 - val_loss: 14.4176\n",
            "Epoch 44/50\n",
            "196/196 [==============================] - 204s 1s/step - loss: 13.9901 - val_loss: 14.8987\n",
            "Epoch 45/50\n",
            "196/196 [==============================] - 204s 1s/step - loss: 13.8256 - val_loss: 14.5467\n",
            "Epoch 46/50\n",
            "196/196 [==============================] - 203s 1s/step - loss: 13.5764 - val_loss: 14.3437\n",
            "Epoch 47/50\n",
            "196/196 [==============================] - 202s 1s/step - loss: 13.8242 - val_loss: 14.2732\n",
            "Epoch 48/50\n",
            "196/196 [==============================] - 200s 1s/step - loss: 13.8901 - val_loss: 14.2579\n",
            "Epoch 49/50\n",
            "196/196 [==============================] - 199s 1s/step - loss: 13.9399 - val_loss: 14.8077\n",
            "Epoch 50/50\n",
            "196/196 [==============================] - 199s 1s/step - loss: 13.7049 - val_loss: 14.5126\n",
            "Unfreeze all of the layers.\n",
            "Train on 6300 samples, val on 700 samples, with batch size 16.\n",
            "Epoch 51/100\n",
            "393/393 [==============================] - 214s 545ms/step - loss: 13.2666 - val_loss: 13.5502\n",
            "Epoch 52/100\n",
            "393/393 [==============================] - 212s 539ms/step - loss: 12.9347 - val_loss: 13.5603\n",
            "Epoch 53/100\n",
            "393/393 [==============================] - 213s 541ms/step - loss: 12.7321 - val_loss: 13.0992\n",
            "Epoch 54/100\n",
            "393/393 [==============================] - 215s 548ms/step - loss: 12.5536 - val_loss: 13.0858\n",
            "Epoch 55/100\n",
            "393/393 [==============================] - 216s 550ms/step - loss: 12.3859 - val_loss: 13.0997\n",
            "Epoch 56/100\n",
            "393/393 [==============================] - 216s 551ms/step - loss: 12.1655 - val_loss: 12.4609\n",
            "Epoch 57/100\n",
            "393/393 [==============================] - 216s 549ms/step - loss: 12.1770 - val_loss: 12.6048\n",
            "Epoch 58/100\n",
            "393/393 [==============================] - 214s 545ms/step - loss: 11.9460 - val_loss: 12.7390\n",
            "Epoch 59/100\n",
            "393/393 [==============================] - 210s 535ms/step - loss: 12.0050 - val_loss: 12.7674\n",
            "Epoch 60/100\n",
            "393/393 [==============================] - 207s 526ms/step - loss: 12.0058 - val_loss: 12.6214\n",
            "Epoch 61/100\n",
            "393/393 [==============================] - 206s 525ms/step - loss: 11.7851 - val_loss: 12.0504\n",
            "Epoch 62/100\n",
            "393/393 [==============================] - 205s 522ms/step - loss: 11.8589 - val_loss: 12.2973\n",
            "Epoch 63/100\n",
            "393/393 [==============================] - 205s 521ms/step - loss: 11.7569 - val_loss: 12.0284\n",
            "Epoch 64/100\n",
            "393/393 [==============================] - 205s 522ms/step - loss: 11.5668 - val_loss: 11.6783\n",
            "Epoch 65/100\n",
            "393/393 [==============================] - 204s 518ms/step - loss: 11.5942 - val_loss: 12.2481\n",
            "Epoch 66/100\n",
            "393/393 [==============================] - 203s 518ms/step - loss: 11.6309 - val_loss: 12.2185\n",
            "Epoch 67/100\n",
            "393/393 [==============================] - 203s 517ms/step - loss: 11.5838 - val_loss: 12.5425\n",
            "Epoch 68/100\n",
            "393/393 [==============================] - 204s 518ms/step - loss: 11.4863 - val_loss: 11.9319\n",
            "Epoch 69/100\n",
            "393/393 [==============================] - ETA: 0s - loss: 11.4222\n",
            "Epoch 00069: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
            "393/393 [==============================] - 202s 515ms/step - loss: 11.4222 - val_loss: 11.9935\n",
            "Epoch 70/100\n",
            "393/393 [==============================] - 203s 518ms/step - loss: 11.5359 - val_loss: 11.7674\n",
            "Epoch 71/100\n",
            "393/393 [==============================] - 204s 520ms/step - loss: 11.3162 - val_loss: 11.5714\n",
            "Epoch 72/100\n",
            "393/393 [==============================] - 207s 527ms/step - loss: 11.2295 - val_loss: 11.8676\n",
            "Epoch 73/100\n",
            "393/393 [==============================] - 204s 520ms/step - loss: 11.2531 - val_loss: 12.1820\n",
            "Epoch 74/100\n",
            "393/393 [==============================] - 205s 521ms/step - loss: 11.1699 - val_loss: 12.0130\n",
            "Epoch 75/100\n",
            "393/393 [==============================] - 209s 532ms/step - loss: 11.3061 - val_loss: 11.7658\n",
            "Epoch 76/100\n",
            "393/393 [==============================] - ETA: 0s - loss: 11.1704\n",
            "Epoch 00076: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "393/393 [==============================] - 211s 536ms/step - loss: 11.1704 - val_loss: 11.8713\n",
            "Epoch 77/100\n",
            "393/393 [==============================] - 211s 537ms/step - loss: 11.3551 - val_loss: 11.8698\n",
            "Epoch 78/100\n",
            "393/393 [==============================] - 210s 533ms/step - loss: 11.1736 - val_loss: 11.7098\n",
            "Epoch 79/100\n",
            "393/393 [==============================] - 209s 532ms/step - loss: 11.2371 - val_loss: 11.8733\n",
            "Epoch 80/100\n",
            "393/393 [==============================] - 212s 540ms/step - loss: 11.1532 - val_loss: 11.6249\n",
            "Epoch 81/100\n",
            "393/393 [==============================] - ETA: 0s - loss: 11.2193\n",
            "Epoch 00081: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
            "393/393 [==============================] - 212s 539ms/step - loss: 11.2193 - val_loss: 11.9093\n",
            "Epoch 00081: early stopping\n",
            "Unfreeze all of the layers.\n",
            "Train on 6300 samples, val on 700 samples, with batch size 8.\n",
            "Unfreeze all of the layers.\n",
            "Train on 6300 samples, val on 700 samples, with batch size 4.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1Ju4Zah2mHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}